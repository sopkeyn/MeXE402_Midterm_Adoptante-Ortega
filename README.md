# ğŒğˆğƒğ“ğ„ğ‘ğŒ: ğğšğ¢ğ«-ğğšğ¬ğğ ğğ«ğ¨ğ£ğğœğ­ ğ›ğ² ğ€ğƒğğğ“ğ€ğğ“ğ„ & ğğ‘ğ“ğ„ğ†ğ€

<img src="https://github.com/user-attachments/assets/8be84541-550c-4449-9992-9ea031b2bd87"  alt="My GIF" width="1100" height="290" />
<img src="https://github.com/user-attachments/assets/a56328b5-b523-48f2-9882-554845c1951b" alt="Logistic Email Header" width="1100" height="290" />

<p align="center">**ğˆğğ“ğ‘ğğƒğ”ğ‚ğ“ğˆğğ: ğğ¯ğğ«ğ¯ğ¢ğğ° ğ¨ğŸ ğ‹ğ¢ğ§ğğšğ« ğšğ§ğ ğ‹ğ¨ğ ğ¢ğ¬ğ­ğ¢ğœ ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§**</p>

---

**Linear Regression** and **Logistic Regression** are foundational statistical techniques used for predicting outcomes based on input data. Linear regression estimates continuous outcomes by fitting a straight line to the relationship between variables, while logistic regression models binary outcomes by estimating the probability that a given input falls into one of two categories using a logistic function. Both methods are essential for data analysis, enabling researchers and practitioners to draw insights and make informed decisions based on their data.

Additionally, **Linear regression** is a widely used statistical method that helps us understand the relationship between one or more independent variables and a continuous dependent variable. Its main goal is to find a straight line that best represents the connection between these variables. This approach allows researchers to predict future outcomes based on historical data. In its simplest form, linear regression looks at the effect of a single predictor variable, but it can also accommodate multiple predictors in more complex analyses. The method works by reducing the differences between actual values and predicted values, known as residuals.

**Several key assumptions underlie linear regression:**

- The relationships between variables are linear.
- The effect of an independent variable on the dependent variable remains consistent.
- Errors (the differences between observed and predicted values) are independent of each other and normally distributed.
- Errors have constant variance (homoscedasticity).

Evaluating the effectiveness of a linear regression model often involves metrics like the coefficient of determination, which indicates how much of the variability in the dependent variable can be explained by the independent variables. Linear regression is commonly applied in various fields such as economics, healthcare, and social sciences, aiding in tasks like forecasting trends. Its simplicity makes it accessible to researchers and practitioners alike, allowing for easy interpretation of results.

On the other hand, **Logistic regression** is a statistical method used to predict binary outcomes, meaning outcomes that can be categorized into two distinct groups, like yes/no or success/failure. This method estimates the likelihood of an event happening based on one or more independent variables. Unlike linear regression, which deals with continuous outcomes, logistic regression focuses on categorical results,making it especially useful for classification tasks. It can analyze multiple independent variables at once, providing insights into how these variables affect the probability of a certain outcome.

**Key assumptions of logistic regression include:**

- Independence of observations.
- A linear relationship between independent variables and the log-odds of the outcome.

    This technique finds application in areas like healthcare for predicting disease occurrences and in marketing for customer segmentation. To evaluate logistic regression models, various performance metrics are used, such as accuracy, precision, recall, and the area under the Receiver Operating Characteristic (ROC) curve.
  
    Overall, **Linear regression** is a foundational statistical technique that models the relationship between independent and continuous dependent variables, enabling predictions based on historical data. In contrast, **Logistic regression** is tailored for binary outcomes, estimating the probability of an event occurring based on one or more predictors. Together, these methods provide valuable insights across various fields, helping researchers and practitioners make informed decisions based on data-driven analyses. Both techniques are essential for understanding complex relationships in data and enhancing predictive accuracy.
  
---

<img src="https://github.com/user-attachments/assets/072bdead-9c53-4936-93e2-88869494b4bb" alt="Header 1" width="1100" height="290" />

**ğ‹ğ¢ğ§ğğšğ« ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§: ğ’ğ­ğ®ğğğ§ğ­ ğğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ ğƒğšğ­ğš**
The Student Performance dataset on Kaggle investigates academic achievement in secondary education across two Portuguese schools. This dataset includes various attributes, such as student grades, demographic information, and social and school-related features, all gathered through school reports and student questionnaires. It consists of two primary datasets focusing on performance in Mathematics (referred to as "mat") and Portuguese language (referred to as "por").

According to research by Cortez and Silva (2008), these datasets can be used for both classification and regression tasks, providing flexibility in analysis. The target variable, G3, represents the final year grade and is closely correlated with the earlier grades G1 and G2, which correspond to the first and second-period assessments. This correlation highlights the challenge of predicting G3 without incorporating G1 and G2, though such predictions are deemed more beneficial for understanding student performance. The dataset serves as an important resource for researchers aiming to analyze educational outcomes and the factors influencing them.

<img src="https://github.com/user-attachments/assets/6480f15a-e1ee-4e62-b852-9a32ea011344" alt="Header 2" width="1100" height="290" />

**ğ‹ğ¨ğ ğ¢ğ¬ğ­ğ¢ğœ ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§: ğ‡ğğšğ«ğ­ ğƒğ¢ğ¬ğğšğ¬ğ ğƒğšğ­ğšğ¬ğğ­**
The Heart Disease dataset, originating from 1988, comprises data from four databases: Cleveland, Hungary, Switzerland, and Long Beach V. It features a total of 76 attributes, although many studies focus on a subset of 14 key attributes for analysis. The dataset aims to predict the presence of heart disease in patients, indicated by the target field. This target is coded as an integer value, where 0 signifies no heart disease and 1 indicates the presence of the disease.

The comprehensive nature of this dataset makes it a valuable tool for healthcare professionals and researchers, as it provides insights into various risk factors associated with cardiovascular health. Its use in logistic regression analyses enables the development of predictive models that can help improve patient outcomes and guide medical decision-making. Overall, the Heart Disease dataset plays a crucial role in advancing research in cardiology and public health.

---

<img src="https://github.com/user-attachments/assets/af254068-afe6-41c3-9f06-22977d292def" alt="Header 3" width="1100" height="290" />

****ğğ«ğ¨ğ£ğğœğ­ ğğ›ğ£ğğœğ­ğ¢ğ¯ğğ¬ ğŸğ¨ğ« ğ‹ğ¢ğ§ğğšğ« ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§ ğ¨ğ§ ğ’ğ­ğ®ğğğ§ğ­ ğğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ ğƒğšğ­ğšğ¬ğğ­**

-To analyze the dataset by conducting a comprehensive exploration of the dataset to understand its structure and identify any missing values, outliers, and necessary preprocessing steps.

-Identify the key fctors by determining significant factors influencing student performance by analyzing the relationships among demographic, social, and academic variables in relation to the final grade (G3).

-Develop a predictive model by creatoing a linear regression model to predict G3 based on relevant independent variables, using training and testing sets and evaluating performance through metrics like RÂ²

-Validate and Test the Model to ensure model robustness through cross-validation and performance assessment using additional metrics for accuracy and reliability.

****ğğ«ğ¨ğ£ğğœğ­ ğğ›ğ£ğğœğ­ğ¢ğ¯ğğ¬ ğŸğ¨ğ« ğ‹ğ¨ğ ğ¢ğ¬ğ­ğ¢ğœ ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§ ğ¨ğ§ ğ‡ğğšğ«ğ­ ğƒğ¢ğ¬ğğšğ¬ğ ğ”ğ‚ğˆ ğƒğšğ­ğšğ¬ğğ­**

-To analyze the dataset to explore the Heart Disease dataset to understand its structure, identify missing values and outliers, and perform necessary preprocessing.

-To identify key risk factors by analyzing medical and demographic factors influencing heart disease using statistical methods and visualizations to uncover correlations.

-Develop a predictive models by creatiing a logistic regression model to predict heart disease presence, splitting the dataset into training and testing sets, and evaluating performance through various metrics.

-Validate and Test the Model: Ensure model robustness through cross-validation and performance assessment using additional metrics for accuracy and reliability.

---

<img src="https://github.com/user-attachments/assets/d1fbb6d0-d50d-4f0b-bfa5-9dc17baeae8f" alt="Header 4" width="1100" height="290" />

### Label Encoding for Categorical Variables

In data analysis and machine learning, it's essential to convert categorical variables into numerical formats so that algorithms can process them effectively. This process is known as **Label Encoding**. Below is a detailed explanation of how I assigned unique numerical values to each category in the dataset:

- **School:**
  - "GP" was encoded as 6.
  - "MS" was encoded as 5.

- **Sex:**
  - "Male" (M) is represented as 1.
  - "Female" (F) is encoded as 0.

- **Age:**
  - Age values (15, 16, 17, 18, 19, and 20) were directly mapped to their respective numerical values.

- **Address:**
  - "Urban" (U) is encoded as 3000.
  - "Rural" (R) is encoded as 3100.

- **Family Size:**
  - "Greater than 3" (GT3) is encoded as 3200.
  - "Less than or equal to 3" (LE3) is encoded as 3300.

- **Parents' Cohabitation Status:**
  - "Together" (T) is encoded as 3400.
  - "Apart" (A) is encoded as 3500.

- **Motherâ€™s Education:**
  - None: 0
  - Primary Education: 1
  - 4th Grade: 2
  - 5th to 9th Grade: 3
  - Secondary: 4
  - College and Tertiary: 4 (both are given the same value for simplicity).

- **Fatherâ€™s Education:**
  - Similar encoding as the motherâ€™s education for consistency.

- **Motherâ€™s Job:**
  - At Home: 3600
  - Health: 3700
  - Other: 3800
  - Teacher: 3900
  - Services: 4000

- **Fatherâ€™s Job:**
  - The same encoding approach was applied for uniformity.

- **Reason for Choosing School:**
  - Course: 4100
  - Home: 4200
  - Other: 3800
  - Reputation: 4300

- **Guardian:**
  - Mother: 4400
  - Father: 4500

- **School Support upto Romantic column:**
  - Yes: 1
  - No: 2

### Summary of Changes
By converting these categorical variables into numerical formats, we enable our dataset to be suitable for various analytical and machine learning tasks. It's crucial to apply these mappings consistently across the dataset to ensure data integrity and reliable analysis. This systematic approach not only prepares the data for model training but also facilitates a clearer understanding of the relationships between variables.

---

### Data Preprocessing

The dataset contains 649 rows and 33 columns. Each column is of integer type, which suggests encoded categorical and continuous data. Some important columns likely include grades (G1, G2, G3), absences, and various attributes related to the student's environment and lifestyle.

![Screenshot 2024-10-29 010009](https://github.com/user-attachments/assets/f423129b-8519-460f-b1fa-09af02ad9aad)


Outliers were identified in this dataset to understand and handle anomalous data points effectively. Hereâ€™s the approach taken:

1. **Outlier Identification**:
   - We used **Z-score analysis** for continuous variables. Observations with a Z-score greater than 3 or less than -3 were flagged as outliers.
   - Outliers were primarily found in the following columns:
     - `traveltime`: 16 outliers
     - `failures`: 14 outliers
     - `famrel`: 22 outliers
     - `Dalc`: 17 outliers
     - `absences`: 11 outliers
     - `G3`: 16 outliers

2. **Outlier Treatment**:
   - Outliers were retained in the dataset but could be removed or capped based on future model needs.
   - **Rationale**: Removing outliers can help in reducing noise, while keeping them may retain information about extreme values.

### Data Normalization

To make the data suitable for machine learning models and analyses sensitive to feature scale, Min-Max normalization was applied.

- **Min-Max Scaling**: This approach scales each feature to a range between 0 and 1, preserving the distribution while ensuring equal weighting across features.

Example of the normalization formula:
\[
X_{normalized} = \frac{X - X_{min}}{X_{max} - X_{min}}
\]

This normalized data is now ready for analysis or modeling, where each feature contributes proportionally.

The linear regression model has been trained to predict the final grade (G3). Here are the evaluation results on the test set:

### Model Implementation

### Evaluation Metrics: Calculate R-squared and Mean Squared Error

### INTERPRETATION: Model's Predictive Power
The predictive power of the model can be assessed using metrics like R-squared and Mean Squared Error (MSE):

**R-squared:**

R-squared indicates the proportion of the variance in the dependent variable that can be explained by the independent variables. An R-squared value close to 1 suggests that the model explains a significant portion of the variability in student performance. For instance, an R-squared of 0.85 means that 85% of the variability in final grades can be accounted for by the predictors in the model.

**Mean Squared Error (MSE):**

MSE provides a measure of the average squared differences between predicted and actual final grades. A lower MSE indicates better predictive accuracy. For example, an MSE of 10 suggests that, on average, the model's predictions deviate from the actual grades by the square root of 10 (approximately 3.16 points).

Conclusion
Understanding the coefficients and the model's predictive power helps educators and stakeholders identify key factors influencing student performance. This knowledge can guide interventions, such as promoting effective study habits or improving attendance policies. Ultimately, leveraging predictive modeling can lead to better educational outcomes by tailoring strategies to support student success.
**Mean Squared Error (MSE): 1.462**
**R-squared (RÂ²): 0.850**
The RÂ² score of **0.85** indicates that the model explains about **85%** of the variance in the final grade (G3). This suggests a reasonably good fit for the data, though there is still some room for improvement.

---






<img src="https://github.com/user-attachments/assets/2ae218db-4d85-4617-8cb8-fedf967a90f4" alt="Header 5" width="1100" height="290" />

