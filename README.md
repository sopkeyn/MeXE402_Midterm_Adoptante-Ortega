# ğŒğˆğƒğ“ğ„ğ‘ğŒ: ğğšğ¢ğ«-ğğšğ¬ğğ ğğ«ğ¨ğ£ğğœğ­ ğ›ğ² ğ€ğƒğğğ“ğ€ğğ“ğ„ & ğğ‘ğ“ğ„ğ†ğ€

<img src="https://github.com/user-attachments/assets/8be84541-550c-4449-9992-9ea031b2bd87"  alt="My GIF" width="1100" height="290" />
<img src="https://github.com/user-attachments/assets/a56328b5-b523-48f2-9882-554845c1951b" alt="Logistic Email Header" width="1100" height="290" />
<p align="center">**ğˆğğ“ğ‘ğğƒğ”ğ‚ğ“ğˆğğ: ğğ¯ğğ«ğ¯ğ¢ğğ° ğ¨ğŸ ğ‹ğ¢ğ§ğğšğ« ğšğ§ğ ğ‹ğ¨ğ ğ¢ğ¬ğ­ğ¢ğœ ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§**</p>

---
**Linear Regression** and **Logistic Regression** are foundational statistical techniques used for predicting outcomes based on input data. Linear regression estimates continuous outcomes by fitting a straight line to the relationship between variables, while logistic regression models binary outcomes by estimating the probability that a given input falls into one of two categories using a logistic function. Both methods are essential for data analysis, enabling researchers and practitioners to draw insights and make informed decisions based on their data.

Additionally, **Linear regression** is a widely used statistical method that helps us understand the relationship between one or more independent variables and a continuous dependent variable. Its main goal is to find a straight line that best represents the connection between these variables. This approach allows researchers to predict future outcomes based on historical data. In its simplest form, linear regression looks at the effect of a single predictor variable, but it can also accommodate multiple predictors in more complex analyses. The method works by reducing the differences between actual values and predicted values, known as residuals. Evaluating the effectiveness of a linear regression model often involves metrics like the coefficient of determination, which indicates how much of the variability in the dependent variable can be explained by the independent variables. Linear regression is commonly applied in various fields such as economics, healthcare, and social sciences, aiding in tasks like forecasting trends. Its simplicity makes it accessible to researchers and practitioners alike, allowing for easy interpretation of results.

On the other hand, **Logistic regression** is a statistical method used to predict binary outcomes, meaning outcomes that can be categorized into two distinct groups, like yes/no or success/failure. This method estimates the likelihood of an event happening based on one or more independent variables. Unlike linear regression, which deals with continuous outcomes, logistic regression focuses on categorical results,making it especially useful for classification tasks. It can analyze multiple independent variables at once, providing insights into how these variables affect the probability of a certain outcome.
  
Overall, **Linear regression** is a foundational statistical technique that models the relationship between independent and continuous dependent variables, enabling predictions based on historical data. In contrast, **Logistic regression** is tailored for binary outcomes, estimating the probability of an event occurring based on one or more predictors. Together, these methods provide valuable insights across various fields, helping researchers and practitioners make informed decisions based on data-driven analyses. Both techniques are essential for understanding complex relationships in data and enhancing predictive accuracy.

---

<img src="https://github.com/user-attachments/assets/072bdead-9c53-4936-93e2-88869494b4bb" alt="Header 1" width="1100" height="290" />

**ğ‹ğ¢ğ§ğğšğ« ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§: ğ’ğ­ğ®ğğğ§ğ­ ğğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ ğƒğšğ­ğš**
The Student Performance dataset on Kaggle examines academic achievement in secondary education for students from two Portuguese schools, focusing on Mathematics ("mat") and Portuguese language ("por"). This dataset, compiled from school records and student questionnaires, contains comprehensive data on student demographics, social factors, and school-related variables, making it highly suitable for both regression and classification tasks.

Research by Cortez and Silva (2008) shows that the target variable, G3 (final year grade), is closely related to G1 and G2, which represent first- and second-period grades. While G3 is predictable using G1 and G2, predicting G3 without these earlier grades can be more insightful for examining broader factors influencing final performance, as it isolates variables like study habits, family background, and attendance. This approach is especially valuable for educational researchers looking to understand the non-grade factors affecting student success.

<img src="https://github.com/user-attachments/assets/6480f15a-e1ee-4e62-b852-9a32ea011344" alt="Header 2" width="1100" height="290" />

**ğ‹ğ¨ğ ğ¢ğ¬ğ­ğ¢ğœ ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§: ğ‡ğğšğ«ğ­ ğƒğ¢ğ¬ğğšğ¬ğ ğƒğšğ­ğšğ¬ğğ­**
The Heart Disease dataset from 1988 is a foundational resource in public health, enabling predictive analysis of cardiovascular risk. It compiles data from four key sourcesâ€”Cleveland, Hungary, Switzerland, and Long Beach Vâ€”and contains 76 attributes, although most analyses focus on a critical subset of 14 attributes. The datasetâ€™s target variable is binary-coded, with 0 indicating no heart disease and 1 indicating the presence of disease, making it ideal for developing classification models.

This dataset exemplifies the power of public health datasets in guiding medical research and decision-making. By applying logistic regression and similar techniques, researchers can identify key risk factors, such as age, cholesterol levels, and blood pressure, which contribute to heart disease. Predictive models built from this dataset help healthcare professionals anticipate patient needs, devise preventive measures, and personalize care plans. As part of broader public health datasets, the Heart Disease dataset supports advancing research in cardiology by enabling insights that inform healthcare policies, clinical practices, and patient management strategies. Its role is pivotal in shaping data-driven interventions in cardiology and contributing to public health improvements.

---

<img src="https://github.com/user-attachments/assets/af254068-afe6-41c3-9f06-22977d292def" alt="Header 3" width="1100" height="290" />

****ğğ«ğ¨ğ£ğğœğ­ ğğ›ğ£ğğœğ­ğ¢ğ¯ğğ¬ ğŸğ¨ğ« ğ‹ğ¢ğ§ğğšğ« ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§ ğ¨ğ§ ğ’ğ­ğ®ğğğ§ğ­ ğğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ ğƒğšğ­ğšğ¬ğğ­**

-To analyze the dataset by conducting a comprehensive exploration of the dataset to understand its structure and identify any missing values, outliers, and necessary preprocessing steps.

-Identify the key fctors by determining significant factors influencing student performance by analyzing the relationships among demographic, social, and academic variables in relation to the final grade (G3).

-Develop a predictive model by creatoing a linear regression model to predict G3 based on relevant independent variables, using training and testing sets and evaluating performance through metrics like RÂ²

-Validate and Test the Model to ensure model robustness through cross-validation and performance assessment using additional metrics for accuracy and reliability.

****ğğ«ğ¨ğ£ğğœğ­ ğğ›ğ£ğğœğ­ğ¢ğ¯ğğ¬ ğŸğ¨ğ« ğ‹ğ¨ğ ğ¢ğ¬ğ­ğ¢ğœ ğ‘ğğ ğ«ğğ¬ğ¬ğ¢ğ¨ğ§ ğ¨ğ§ ğ‡ğğšğ«ğ­ ğƒğ¢ğ¬ğğšğ¬ğ ğ”ğ‚ğˆ ğƒğšğ­ğšğ¬ğğ­**

-To analyze the dataset to explore the Heart Disease dataset to understand its structure, identify missing values and outliers, and perform necessary preprocessing.

-To identify key risk factors by analyzing medical and demographic factors influencing heart disease using statistical methods and visualizations to uncover correlations.

-Develop a predictive models by creatiing a logistic regression model to predict heart disease presence, splitting the dataset into training and testing sets, and evaluating performance through various metrics.

-Validate and Test the Model: Ensure model robustness through cross-validation and performance assessment using additional metrics for accuracy and reliability.

---

<img src="https://github.com/user-attachments/assets/d1fbb6d0-d50d-4f0b-bfa5-9dc17baeae8f" alt="Header 4" width="1100" height="290" />

# ğ’ğ­ğ®ğğğ§ğ­ ğğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ ğƒğšğ­ğš

### Label Encoding for Categorical Variables

In data analysis and machine learning, it's essential to convert categorical variables into numerical formats so that algorithms can process them effectively. This process is known as **Label Encoding**. Below is a detailed explanation of how I assigned unique numerical values to each category in the dataset:

- **School:**
  - "GP" was encoded as 6.
  - "MS" was encoded as 5.

- **Sex:**
  - "Male" (M) is represented as 1.
  - "Female" (F) is encoded as 0.

- **Age:**
  - Age values (15, 16, 17, 18, 19, and 20) were directly mapped to their respective numerical values.

- **Address:**
  - "Urban" (U) is encoded as 3000.
  - "Rural" (R) is encoded as 3100.

- **Family Size:**
  - "Greater than 3" (GT3) is encoded as 3200.
  - "Less than or equal to 3" (LE3) is encoded as 3300.

- **Parents' Cohabitation Status:**
  - "Together" (T) is encoded as 3400.
  - "Apart" (A) is encoded as 3500.

- **Motherâ€™s Education:**
  - None: 0
  - Primary Education: 1
  - 4th Grade: 2
  - 5th to 9th Grade: 3
  - Secondary: 4
  - College and Tertiary: 4 (both are given the same value for simplicity).

- **Fatherâ€™s Education:**
  - Similar encoding as the motherâ€™s education for consistency.

- **Motherâ€™s Job:**
  - At Home: 3600
  - Health: 3700
  - Other: 3800
  - Teacher: 3900
  - Services: 4000

- **Fatherâ€™s Job:**
  - The same encoding approach was applied for uniformity.

- **Reason for Choosing School:**
  - Course: 4100
  - Home: 4200
  - Other: 3800
  - Reputation: 4300

- **Guardian:**
  - Mother: 4400
  - Father: 4500

- **School Support upto Romantic column:**
  - Yes: 1
  - No: 2

### Summary of Changes
By converting these categorical variables into numerical formats, we enable our dataset to be suitable for various analytical and machine learning tasks. It's crucial to apply these mappings consistently across the dataset to ensure data integrity and reliable analysis. This systematic approach not only prepares the data for model training but also facilitates a clearer understanding of the relationships between variables.

---

### ğƒğšğ­ğš ğğ«ğğ©ğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ 

The dataset contains 649 rows and 33 columns. Each column is of integer type, which suggests encoded categorical and continuous data. Some important columns likely include grades (G1, G2, G3), absences, and various attributes related to the student's environment and lifestyle.

![Screenshot 2024-10-29 010009](https://github.com/user-attachments/assets/f423129b-8519-460f-b1fa-09af02ad9aad)


Outliers were identified in this dataset to understand and handle anomalous data points effectively. Hereâ€™s the approach taken:

1. **Outlier Identification**:
   - We used **Z-score analysis** for continuous variables. Observations with a Z-score greater than 3 or less than -3 were flagged as outliers.
   - Outliers were primarily found in the following columns:
     - `traveltime`(travel time to school): 16 outliers. Extreme travel times might affect student performance due to fatigue or limited study time.
     - `failures`(number of past class failures): 14 outliers. High numbers of failures indicate students who may be struggling significantly.
     - `famrel`(quality of family relationships): 22 outliers. Outliers here can reflect extreme family dynamics, impacting mental health and performance.
     - `Dalc` (workday alcohol consumption): 17 outliers. High or low alcohol consumption levels could influence focus and cognitive function.
     - `absences`(number of school absences): 11 outliers. Higher absences could lead to poor academic performance due to missed content.
     - `G3`(final grade): 16 outliers. Low or exceptionally high grades are notable as they reflect extreme performance levels, both good and bad.

2. **Outlier Treatment**:
   - Outliers were retained in the dataset but could be removed or capped based on model requirements.
   - **Rationale**: taining outliers may provide insights into extreme behaviors that influence academic success or failure. Removing them, however, could reduce noise in the dataset for more accurate modeling 
 results. Future adjustments could include capping extreme values or excluding them based on the modelâ€™s sensitivity to outliers.
     
---  

### ğŒğ¨ğğğ¥ ğˆğ¦ğ©ğ¥ğğ¦ğğ§ğ­ğšğ­ğ¢ğ¨ğ§
![Screenshot 2024-10-29 010536](https://github.com/user-attachments/assets/a51220d6-7d34-446d-931d-d3ce62a2adad)

The `.fit()` method is used to train or â€œfitâ€ the model. It takes in two arguments:

- `X_train`: the training data's feature set (independent variables).
- `y_train`: the training dataâ€™s target variable (dependent variable).

- The `.predict()` method uses the trained model to predict outcomes based on the `X_test` data.
- This method outputs predictions (stored in `y_pred`) for each observation in `X_test`.
  

- This array represents the modelâ€™s predicted output values for each corresponding test example:

  ![Screenshot 2024-10-29 010627](https://github.com/user-attachments/assets/15058142-6056-423f-9d77-af606c85cd8b)
  
In this code, we are making a prediction for a single data point represented by a list of feature values. Each feature corresponds to a specific attribute (like age, address, parental education, family support, etc.) relevant to the modelâ€™s prediction.

**Making a prediction for a single data point with feature values**.
This feature list contains of the values representing various characteristics for a single student, such as:

ğƒğğŸğ¢ğ§ğ ğ­ğ¡ğ ğŸğğšğ­ğ®ğ«ğ ğ¬ğğ­ (ğ¢ğ§ğğğ©ğğ§ğğğ§ğ­ ğ¯ğšğ«ğ¢ğšğ›ğ¥ğğ¬)
X_train = [

    '6',    # sx: student's sex 
    '0',    # age: student's age
    '18',   # address
    '3000', # family support
    '3200', # parental support
    '3500', # Medu: mother's education level
    '4',    # Fedu: father's education level
    '4',    # Mjob: mother's job
    '3600', # Fjob: father's job
    '3900', # reason for choosing school
    '4100', # guardian
    '4400', # travel time
    '2',    # study time
    '2',    # failures
    '0',    # schoolsup: school support
    '1',    # famsup: family support
    '2',    # paid tutoring
    '2',    # extracurricular activities
    '2',    # nursery attended
    '1',    # higher education aspiration
    '1',    # internet access
    '2',    # romantic relationship
    '2',    # family relations
    '4',    # free time after school
    '3',    # frequency of going out
    '4',    # weekday alcohol consumption
    '1',    # weekend alcohol consumption
    '1',    # health status
    '3',    # number of school absences
    '4',    # G1: first period grade
    '0'     # G2: second period grade
]

ğ“ğšğ«ğ ğğ­ ğ¯ğšğ«ğ¢ğšğ›ğ¥ğ (ğğğ©ğğ§ğğğ§ğ­ ğ¯ğšğ«ğ¢ğšğ›ğ¥ğ)

y_train = 11  # G3: final grade

**Predicting the outcome for the single student**

single_prediction = model.predict([single_student_features])

print("Predicted outcome:", single_prediction)


![Screenshot 2024-10-29 010651](https://github.com/user-attachments/assets/499d83d5-f0d8-4bc4-9d41-595cd80a0847)

---

### ğ„ğ¯ğšğ¥ğ®ğšğ­ğ¢ğ¨ğ§ ğŒğğ­ğ«ğ¢ğœğ¬: ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ‘-ğ¬ğªğ®ğšğ«ğğ ğšğ§ğ ğŒğğšğ§ ğ’ğªğ®ğšğ«ğğ ğ„ğ«ğ«ğ¨ğ«

![Screenshot 2024-10-29 010914](https://github.com/user-attachments/assets/fe28eaae-c01b-4704-b3c5-f384d509facc)

**R-squared:**

Calculate R-squared (RÂ²)
Assuming `y_test` contains the true values and `y_pred` contains the predicted values from your model

**1. R-squared Calculation**
r2 = r2_score(y_test, y_pred)
print("R-squared:", r2)

Explanation:
R-squared (RÂ²) measures the proportion of the variance in the dependent variable (y) that is predictable from the independent variables (X). It ranges from 0 to 1, with values close to 1 indicating a strong correlation between the observed values and the predicted values, meaning the model explains a large portion of the variability in the data.

if **RÂ² = 0.8566**, it suggests that approximately **85.66%** of the variance in the dependent variable can be explained by the model.

**2. Adjusted R-squared Calculation**

n = X_test.shape[0]  # Number of observations (rows in X_test)

k = X_test.shape[1]  # Number of features (columns in X_test)

adj_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)


Explanation:
Adjusted R-squared adjusts the RÂ² value by taking into account the number of features (k) in the model. This metric helps mitigate overfitting, as it penalizes the addition of features that do not contribute meaningfully to the model. Unlike RÂ², Adjusted RÂ² can decrease if a new feature doesn't improve the model.

if **adj_r2 = 0.8093**, it suggests that about **80.93%** of the variance in y is explained by the model, adjusted for the number of features.

**3. Mean Squared Error Calculation (MSE):**

MSE provides a measure of the average squared differences between predicted and actual final grades. A lower MSE indicates better predictive accuracy. For example, an MSE of 10 suggests that, on average, the model's predictions deviate from the actual grades by the square root of 10 (approximately 3.16 points).

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error (MSE):", mse)

Explanation:
Mean Squared Error (MSE) calculates the average of the squared differences between the actual values (y) and the predicted values (y_pred). It provides an indication of the model's prediction accuracy, with lower MSE values suggesting better model performance. MSE is in the same units as the square of the dependent variable, making it sensitive to outliers due to the squaring of errors.

---

### ğˆğğ“ğ„ğ‘ğğ‘ğ„ğ“ğ€ğ“ğˆğğ: ğŒğ¨ğğğ¥'ğ¬ ğğ«ğğğ¢ğœğ­ğ¢ğ¯ğ ğğ¨ğ°ğğ«

The predictive power of the model can be assessed using metrics like R-squared and Mean Squared Error (MSE):

Understanding the coefficients and the model's predictive power helps educators and stakeholders identify key factors influencing student performance. This knowledge can guide interventions, such as promoting effective study habits or improving attendance policies. Ultimately, leveraging predictive modeling can lead to better educational outcomes by tailoring strategies to support student success.

**Mean Squared Error (MSE): 1.462**

Interpretation of MSE Value (1.462)
An MSE of 1.462 indicates that, on average, the square of the difference between actual and predicted values is 1.462 units. The squaring part is essential because it penalizes larger errors more heavily than smaller ones. However, since this value is squared, it's more challenging to interpret directly in terms of the actual student performance scale (e.g., grades on a 0-100 scale).

To interpret this MSE, we could consider the following:

Lower MSE: A lower MSE implies that the model predictions are closer to the actual values, signifying better predictive accuracy.

Higher MSE: A higher MSE indicates larger average errors between predictions and actual values, meaning the model isnâ€™t as accurate.

In a clearer sense of the modelâ€™s prediction error in actual performance units, the square root of the MSE to obtain the Root Mean Squared Error (RMSE):

- **Mean Squared Error (MSE)**: The MSE value for the model is **1.462**. MSE is calculated as the average of the squared differences between actual and predicted values, providing insight into the accuracy of the model's predictions.

- **Root Mean Squared Error (RMSE)**: To better understand the predictive accuracy in the same units as the original data, we calculate the RMSE, which is the square root of the MSE. 

This RMSE tells that, on average, the modelâ€™s predictions differ from actual performance scores by approximately 1.21 units. This is easier to interpret since it directly represents the error in the same units as the performance scores.

RMSE= MSE = 1.462 â‰ˆ1.21

Root Mean Squared Error (RMSE): 1.2101531590175027

**R-squared (RÂ²): 0.850**

The RÂ² score of **0.85** indicates that the model explains about **85%** of the variance in the final grade (G3). This suggests a reasonably good fit for the data, though there is still some room for improvement.

---


<img src="https://github.com/user-attachments/assets/2ae218db-4d85-4617-8cb8-fedf967a90f4" alt="Header 5" width="1100" height="290" />

# ğ‡ğğšğ«ğ­ ğƒğ¢ğ¬ğğšğ¬ğ ğƒğšğ­ğšğ¬ğğ­

The correlation analysis reveals key insights into factors impacting students' final grades (G3):

-High Correlation with Previous Grades: Both G1 and G2 grades are strongly positively correlated with the final grade (G3), indicating consistent academic performance.

-Positive Family Influence: Parental education (Medu and Fedu) and study time are positively correlated with G3, suggesting that family support and study time are linked to better outcomes.

-Negative Behavioral Factors: Alcohol consumption (both daily and weekend use, Dalc and Walc) and failures have negative correlations with G3, implying that these behaviors might impact student performance negatively.

![student_performance_heatmap](https://github.com/user-attachments/assets/be4775e7-7325-4f28-951d-a4679c1679ea)

**Methodology**
The analysis utilizes correlation and visualization to identify patterns and relationships between student background, behavioral factors, and academic performance.

Findings Summary Table

![student_performance_table](https://github.com/user-attachments/assets/4b94ce2e-33e3-44b8-8736-ec75364d927c)


**Discussion**
These results highlight the role of family education, consistent academic performance, and behavioral habits on academic outcomes. However, this analysis may be limited by potential data inconsistencies across merged files and lack of control for external variables like socioeconomic factors.

--- 

# Project Analysis

This analysis focuses on **Student Performance** and **Heart Disease** datasets, using linear and logistic regression to identify significant predictors and key insights.

## Methodology

- **Data Preprocessing**:
  - Handled missing values, encoded categorical variables, and standardized numerical features.
  
- **Feature Selection**:
  - Selected relevant features based on correlations and domain knowledge.
  
- **Model Selection and Training**:
  - **Student Performance**: Linear regression to predict grades.
  - **Heart Disease**: Logistic regression for predicting disease presence.

- **Model Evaluation**:
  - For linear regression: Evaluated with R-squared, MAE, and RMSE.
  - For logistic regression: Assessed using accuracy, precision, recall, F1-score, and AUC-ROC.

---

## Findings

### ğ’ğ­ğ®ğğğ§ğ­ ğğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ ğƒğšğ­ğšğ¬ğğ­ (Linear Regression)

- **Predictive Variables**: Factors like study time, parental education, and previous grades show a significant relationship with final grades.
- **Key Findings**: 
  - Predictors such as study time and parental involvement positively impact grades.
  - Absenteeism and frequent alcohol use have negative impacts on performance.

### ğ‡ğğšğ«ğ­ ğƒğ¢ğ¬ğğšğ¬ğ ğƒğšğ­ğšğ¬ğğ­ (Logistic Regression)

- **Predictive Variables**: Features such as age, cholesterol levels, blood pressure, and exercise-induced angina impact heart disease risk.
- **Key Findings**: 
  - Higher age, resting blood pressure, and cholesterol correlate with a higher likelihood of heart disease.
  - Physical exercise and lower blood pressure may be protective factors.

---

## Insights Gained

### ğ’ğ­ğ®ğğğ§ğ­ ğğğ«ğŸğ¨ğ«ğ¦ğšğ§ğœğ:

- Socioeconomic factors (e.g., parental education) have a significant influence on academic success.
- Behavioral factors, like study habits and extracurricular activities, contribute meaningfully to performance, indicating a need for tailored academic support.

### ğ‡ğğšğ«ğ­ ğƒğ¢ğ¬ğğšğ¬ğ:

- The impact of lifestyle choices (smoking, diet, exercise) on heart disease risk is significant.
- Identifying high-risk individuals can support early intervention by addressing modifiable factors such as cholesterol and exercise habits.
